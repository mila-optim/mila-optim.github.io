---
layout: post
title: Winter 2023
---
## Goal

The goal of this version of the crash course is to present standard proof techniques for simple optimization algorithms.  
It is tailored for practitioners who use optimization as black-box tools, e.g., SGD or ADAM to optimize neural networks and want to understand the underlying principles of optimization.  
The overall idea is to present each building block of the ADAM optimizer.

For each lecture, proofs will be done on board, and at least 30 minutes will be dedicated to redoing the proofs by yourself. Lecture notes will be released on the fly.

## When & Where

**Wednesday 15:00-17:00, room H04 (Mila 6650 building).**

## Organizers

Danilo Vucetic, Lucas Maes, Damien Scieur, and Quentin Bertrand

## Schedule

The first sessions, from basic concepts to stochastic gradient descent, will be lecture-based, and the last ones will be seminar-based sessions.

- **02-14-2024** Basic concepts, convexity, smoothness, convergence proof of gradient descent (Lecturer: Quentin Bertrand)  
  - **References:** Lecture notes from [Sham Kakade](https://courses.cs.washington.edu/courses/cse546/15au/lectures/lecture09_optimization.pdf), [Ryan Tibshirani](https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf), [Ioannis Mitliagkas](https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-2-notes.pdf), [Robert Gower](https://perso.telecom-paristech.fr/rgower/pdf/M2_statistique_optimisation/grad_conv.pdf), [Mathurin Massias](https://mathurinm.github.io/assets/2022_ens/class.pdf) (Chapters 2 and 3). You can also check [Yuri Nesterov](https://pages.cs.wisc.edu/~yliang/cs839_spring22/material/Introductory-Lectures-on-Convex-Programming-Yurii-Nesterov-2004.pdf)'s book (Chapters 1 and 2).

- **02-21-2024** Subgradient Descent and Dual averaging (Lecturer: Damien Scieur)  
  - **References:** Lectures notes from [Jiantao Jiao](https://people.eecs.berkeley.edu/~jiantao/227c2022spring/scribe/227C_Lecture_04.pdf)

- **02-28-2024** Convergence proof of gradient and subgradient descent Part 2 (Lecturer: Quentin Bertrand)

- **03-06-2024** Acceleration: Nesterov and heavy ball (Lecturer: Damien Scieur)  
  - **References:** Chapter 4 of [Damien's monograph](https://www.nowpublishers.com/article/DownloadSummary/OPT-036)

- **03-13-2024** Stochastic gradient descent (Lecturer: Quentin Bertrand)  
  - **References:** Fabian Pedregosa blog posts for [intuitions](https://fa.bianp.net/teaching/2018/COMP-652/stochastic_gradient.html) and [proofs](https://fa.bianp.net/blog/2021/exponential-sgd/), [Lieven Vandenberghe Lecture's notes](http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf), Simon Lacoste-Julien Lecture notes on [stochastic subgradient method](https://www-labs.iro.umontreal.ca/~slacoste/teaching/ift6132/W24/notes/lecture10_scribbles.pdf) and [convex optimization](https://www-labs.iro.umontreal.ca/~slacoste/teaching/ift6132/W24/notes/lecture11_scribbles.pdf), [Guillaume Garrigos and Robert Gower monography](https://arxiv.org/pdf/2301.11235.pdf)

- **03-20-2024** Adaptive methods: line search and Polyak step size (Lecturer: Damien Scieur)

- **03-27-2024** RMSProp and ADAM (Lecturer: Charles Guille-Escuret)

- **04-17-2024** Automated proof techniques (Lecturer: Gauthier Gidel)

- **04-24-2024** New adaptive techniques for deep learning (Lecturer: Damien Scieur)
